---
title: 'Homework R #9 (Токенизация и лемматизация)'
author: "Latypov Gleb"
date: "2023-12-06"
output: html_document
bibliography: /Users/gleb/Desktop/R/mybib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<div align="center"> 
# Компьютерный анализ текста
</div>


#### _«Финансист» — роман американского писателя Теодора Драйзера, опубликованный в 1912 году, первый из цикла «Трилогия желания»._
&nbsp;


![](https://i5.walmartimages.com/seo/The-Financier-Illustrated-Paperback-9798731304580_23e5ef42-1911-4ea3-bdc9-5e3e5e66c158.3d53e6ea764d3aec9d3215ee02057986.jpeg){ width="400" height="550" style="display: block; margin: 0 auto" }

<div align="center">[Читать оригинал романа](https://www.gutenberg.org/ebooks/1840)</div>

&nbsp;

# Токенизация 

> В рамках компьютерного анализа текста мною был выбран роман Теодора Драйзера "Финансист". С оригиналом произведения вы можете ознакомиться на сайте [gutenberg](https://www.gutenberg.org/). Текст был спарсен с данного сайта в формате .txt.
Он был токенезирован и занесен в таблицу "token_theodore" c колонками "chapter" и "word". Ознакомиться с результатами преобразованной таблицей, а также с кодом вы можете ниже.

```{r warning=FALSE, message=FALSE}
#используемые библиотеки
library(tidytext)
library(tokenizers)
library(dplyr)
library(stringr)
library(udpipe)
library(ggplot2)
library(rvest)
library(wordcloud2)
```

```{r warning=FALSE}
#Загрузим данные
#подвязываем url с Драйзером и парсим сайт с ссылкой
url <- read_html("https://www.gutenberg.org/ebooks/1840")
#находим ссылку "Plain Text UTF-8"
link <- url %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  str_subset("1840.txt") %>%
  paste("https://www.gutenberg.org", ., sep="")
#считываем текст
text <- readLines(link)

#создаем таблицу "theodore" с текстом романа Драйзера и удаляем первые 40  и последние 397 строчек (там содержится мусор)
theodore <- tibble(Text_novel = text)
theodore <- theodore %>% 
  slice(41:n()) %>% 
  slice(1:(nrow(theodore)-398))

#создадим нетокинезированную таблицу для БУДУЩЕЙ лематизации
theo_for_lemma <- theodore %>% 
  mutate(chapter = cumsum(str_detect(Text_novel, "^Chapter [\\DIVXLC]")))

#модифицируем таблицу "theodore" и добавим второй столбец с "chapter"
#(регулярка ищет нам все chapter)
theodore <- theodore %>% 
  mutate(Text_novel = str_to_lower(Text_novel),
         chapter = cumsum(str_detect(Text_novel, "^chapter [\\divxlc]")))

#создаем еще таблицу в которой строки уже разбиты на токены по одному слову
token_theodore <- theodore %>% 
  unnest_tokens(word, Text_novel)
```

&nbsp;

### Первые 10 результатов таблицы

<div align="center"> 
```{r warning=FALSE, echo=FALSE}
head(token_theodore, 10)
```
</div>

&nbsp;

### Получение результатов

>Далее поставленной целью было найти словесные 2-грамм, которые встречаются чаще всего на протяжении всего романа. Для хорошей визуализации было решено отобрать топ-10 и построить диаграму.

```{r warning=FALSE}
#найдем словесные 2-грамм встречающиеся чаще всего на протяжении всего романа
grams2_theodore <- theodore %>% 
  unnest_tokens(ngrams, Text_novel, token = 'ngrams', n = 2) %>% 
  filter(!is.na(ngrams)) %>% 
  count(ngrams) %>% 
  arrange(-n)

top_n <- grams2_theodore %>% 
  top_n(10, n)
```

```{r eval=FALSE, warning=FALSE}
#строим диаграмму 
ggplot(data = top_n, aes(x = reorder(ngrams, n), y = n, fill = ngrams)) +
  geom_bar(stat = "identity") +
  coord_flip()+
  labs(x = '2-grams', y = 'amount') +
  ggtitle("top-10 2-grams in The Financier")
```

&nbsp;


<div align="center"> 
```{r echo=FALSE, warning=FALSE}
ggplot(data = top_n, aes(x = reorder(ngrams, n), y = n, fill = ngrams)) +
  geom_bar(stat = "identity") +
  coord_flip()+
  guides(fill = FALSE) +
  labs(x = '2-grams', y = 'amount') +
  ggtitle("top-10 2-grams in The Financier")
```
</div>

<div align="center"> 
*Самым частотным оказывается "of the", который встречается 1095 раз*
</div>

&nbsp;

# Лематизация 

>Для усвоения материала лекции мы осуществим процесс лематизации над первой главой рома Драйзера.

```{r warning=FALSE, message=FALSE}
#Лематизация 1 главы
#Используем ранее созданную нетокенизированную таблицу 'theo_for_lemma'
lema_theodore <- theo_for_lemma %>% 
  filter(chapter == 1)
#скачаем модель "eng-gum"
#udpipe_download_model(language = "english-gum")
eng_gum <- udpipe_load_model(file = "english-gum-ud-2.5-191206.udpipe")
financier_ann <- udpipe_annotate(eng_gum, x = lema_theodore$Text_novel)

tbl_financier_lema <- as_tibble(financier_ann) 
```

&nbsp;

### Получение результатов

<mark>В качестве модели была взята **"english-gum"** т.к. она обучена на подходящих данных.</mark>

<mark>Данная таблица полностью интерактивная :)</mark>

```{r eval=FALSE}
#первые 10 строчек таблицы
DT::datatable(
  tbl_financier_lema, class = 'cell-border stripe'
  )
```
&nbsp;
```{r echo=FALSE}
#показать первые 10 строчек таблицы
DT::datatable(
  tbl_financier_lema, class = 'cell-border stripe'
  )
```

&nbsp;

>Предлагаю получить интересные данные из таблицы. Найдем топ-10 самых
часто встречающиеся имен собственных (PROPN) из лематизированной таблицы (предположительно это 'Frank', 'Cowperwood', 'Butler').

```{r warning=FALSE, message=FALSE}
#Используем ранее созданную  нетокинезированную таблицу 'theo_for_lemma'
#и подгоним все главы под лематизацию
all_lema_theodore <- theo_for_lemma 
eng_gum <- udpipe_load_model(file = "english-gum-ud-2.5-191206.udpipe")
all_chapters_financier_ann_ <- udpipe_annotate(eng_gum, x = all_lema_theodore$Text_novel)

#преобразуем все в тиббл
tbl_all_chapters_lemma <- as_tibble(all_chapters_financier_ann_) 

#отберем леммы с PROPN
PROPN <- tbl_all_chapters_lemma %>%
  filter(upos == 'PROPN') %>% 
  count(lemma) %>% 
  arrange(-n) %>% 
  top_n(13) %>% 
#удалилим ненужные артефакты
  filter(lemma != 'Mr' & lemma != 'Mr.' & lemma !=  'Street')
```
<div align="center"> 
```{r warning=FALSE, echo=FALSE}
PROPN
```
</div>
>Отлично, мы получили таблицу, из которой можно сделать вывод,
что наше предположение оказалось почти верным. Самым частотным является
фамилия главного героя 'Cowperwood', за ним идет 'Butler' -
фамилия жены Фрэнка - Эйлин(и фамилия ее отца). Stener оказывается на третьем месте - фигура, которая играет важную роль в финансовых махинациях Фрэнка. 

>Для красоты и визуального представления отобразим данные на облаке слов.

```{r eval=FALSE, warning=FALSE}
#построим облако слов
m_colors = c('#730071',"#052F5F", '#FFE900',"#114B5F", "#F34213",'#4A4A48','#5386E4', '#6B2737','#3E2F5B')
b_background = c('#DDFFF7')
wordcloud2(PROPN,
           color = rep_len(m_colors, nrow(PROPN)),
           backgroundColor = b_background,
           size = 0.6)
```

<div align="center"> 
```{r echo=FALSE, warning=FALSE}
#построим облако слов
m_colors = c('#730071',"#052F5F", '#FFE900',"#114B5F", "#F34213",'#4A4A48','#5386E4', '#6B2737','#3E2F5B')
b_background = c('#DDFFF7')
wordcloud2(PROPN,
           color = rep_len(m_colors, nrow(PROPN)),
           backgroundColor = b_background,
           size = 0.6)
```
</div>

&nbsp;

# Частеречная разметка

>Напоследок давайте выполним частеречную разметку и посмотрим на результаты использования частей речи в романе.


```{r warning=FALSE}
#обратимся к "tbl_all_chapters_lemma" со всеми главами и леммами, а также уберем от туда пунктуацию
markup <- tbl_all_chapters_lemma %>% 
  group_by(upos) %>% 
  count() %>% 
  filter(upos != "PUNCT") %>% 
  arrange(-n)
```
<div align="center"> 
```{r echo=FALSE, warning=FALSE, message=FALSE}
markup
```
</div>

>Итого, глаголы занимают 3 место по частотности, а вот существительные употребляются 32,543 раз. В то время как количество междометий относительно существительных ничтожно мало(всего 565).

>Сделаем диаграмму по полученным данным

```{r eval=FALSE,warning=FALSE}
ggplot(data = markup, aes(x = reorder(upos, n), y = n, fill = upos)) +
  geom_bar(stat = "identity") +
  coord_flip()+
  guides(fill = FALSE) +
  labs(x = 'part of speech', y = 'total') +
  ggtitle('Частеречная разметка текста')
```

&nbsp;

<div align="center"> 
```{r echo=FALSE, warning=FALSE}
ggplot(data = markup, aes(x = reorder(upos, n), y = n, fill = upos)) +
  geom_bar(stat = "identity") +
  coord_flip()+
  guides(fill = FALSE) +
  labs(x = 'part of speech', y = 'total') +
  ggtitle('Частеречная разметка текста')
```
</div>

&nbsp;

# Заключение

>Таким образом мы подошли к концу данной работы. Надеюсь вы узнали много интересного из сегодняшнего небольшого исследования. 
Если вы захотите повторить проделанную работу, то можете заглянуть ко мне на [github](https://github.com/ostpanther/text_analytics). Там  вы найдете файл.rmd с кодом.

*Любители рецензий и статей о книгах могут посмотреть интересную статью* [@dreiser2010]


